{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOwbvbHa7CJ/YJKfsaeoin",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevinmatz/AutogenExperiments/blob/main/Proofreader/AutogenTest2_Automated_Proofreading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, install \"pyautogen\":"
      ],
      "metadata": {
        "id": "_yCzVy5W0DbF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxM4kd90x1ua",
        "outputId": "8477fbd8-5498-4774-a134-f01c0e44b70d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyautogen~=0.1.0\n",
            "  Downloading pyautogen-0.1.10-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache (from pyautogen~=0.1.0)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flaml (from pyautogen~=0.1.0)\n",
            "  Downloading FLAML-2.1.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.2/295.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai (from pyautogen~=0.1.0)\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv (from pyautogen~=0.1.0)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from pyautogen~=0.1.0) (2.3.0)\n",
            "Requirement already satisfied: NumPy>=1.17.0rc1 in /usr/local/lib/python3.10/dist-packages (from flaml->pyautogen~=0.1.0) (1.23.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai->pyautogen~=0.1.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai->pyautogen~=0.1.0) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai->pyautogen~=0.1.0) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai->pyautogen~=0.1.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai->pyautogen~=0.1.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai->pyautogen~=0.1.0) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai->pyautogen~=0.1.0) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->pyautogen~=0.1.0) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->pyautogen~=0.1.0) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->pyautogen~=0.1.0) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->pyautogen~=0.1.0) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->pyautogen~=0.1.0) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->pyautogen~=0.1.0) (1.3.1)\n",
            "Installing collected packages: python-dotenv, flaml, diskcache, openai, pyautogen\n",
            "Successfully installed diskcache-5.6.3 flaml-2.1.1 openai-0.28.1 pyautogen-0.1.10 python-dotenv-1.0.0\n"
          ]
        }
      ],
      "source": [
        "%pip install pyautogen~=0.1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTANT: In Google Colab, click the \"Files\" icon in the left-hand panel and in the /content/ folder, upload a OAI_CONFIG_LIST.json file that has the OpenAI key in it. Format of this file:\n",
        "\n",
        "[\n",
        "    {\n",
        "        \"model\": \"gpt-4\",\n",
        "        \"api_key\": \"<secret key>\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "57wah_Cny9ev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the autogen program with the agents and the prompt:"
      ],
      "metadata": {
        "id": "VGwpOX0Bz_Cw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen import AssistantAgent, UserProxyAgent, config_list_from_json\n",
        "\n",
        "# Load LLM inference endpoints from an env variable or a file\n",
        "# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints\n",
        "# and OAI_CONFIG_LIST_sample\n",
        "config_list = config_list_from_json(env_or_file=\"/content/OAI_CONFIG_LIST.json\")\n",
        "\n",
        "assistant = AssistantAgent(\"assistant\", llm_config={\"config_list\": config_list})\n",
        "\n",
        "user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config={\"work_dir\": \"coding\"})\n",
        "\n",
        "# This initiates an automated chat between the two agents to solve the task\n",
        "user_proxy.initiate_chat(assistant, message=\"Please write a command-line program that uses OpenAI’s API to proofread a document. This program should take a Microsoft Word document as input (we can assume the document does not have any complicated layout or images), convert it to Markdown format, break the Markdown document into chunks of text small enough to submit to GPT-4, and then it should submit each chunk to GPT-4 with the request to proofread it ('please perform copy editing on this text to correct spelling, grammar, usage, and punctuation mistakes'), and then reassemble the proofread chunks into a single Markdown document, and then convert it to a Microsoft Word document as output.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0VbGhHlrcHx",
        "outputId": "eda16d41-267a-4723-8da0-fd20fdddd739"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user_proxy (to assistant):\n",
            "\n",
            "Please write a command-line program that uses OpenAI’s API to proofread a document. This program should take a Microsoft Word document as input (we can assume the document does not have any complicated layout or images), convert it to Markdown format, break the Markdown document into chunks of text small enough to submit to GPT-4, and then it should submit each chunk to GPT-4 with the request to proofread it ('please perform copy editing on this text to correct spelling, grammar, usage, and punctuation mistakes'), and then reassemble the proofread chunks into a single Markdown document, and then convert it to a Microsoft Word document as output.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "assistant (to user_proxy):\n",
            "\n",
            "Such a program would include several different steps, and would integrate with several different libraries. Here’s a brief outline of a program that accomplishes this task:\n",
            "\n",
            "1. Convert Word to Markdown with Mammoth.\n",
            "2. Break the Markdown document into small chunks.\n",
            "3. Use OpenAI's GPT-4 API to proofread each chunk.\n",
            "4. Collect the chunks and construct a new Markdown document.\n",
            "5. Convert the proofread Markdown back to Word with pandoc.\n",
            "\n",
            "To use mammoth, you would first need to install it via pip. The same goes for OpenAI's python client, which can be used for the GPT-4 interaction, and for pandown which would be used to convert the final proofread markdown document back to docx.\n",
            "\n",
            "# Filename: proofread.py\n",
            "\n",
            "```python\n",
            "\n",
            "import os\n",
            "import openai\n",
            "import mammoth\n",
            "import textwrap\n",
            "import pandoc\n",
            "\n",
            "\n",
            "# Set the OpenAI API key\n",
            "openai.api_key = 'YOUR_OPENAI_API_KEY'\n",
            "\n",
            "# Convert Word to Markdown\n",
            "with open(\"input.docx\", \"rb\") as docx_file:\n",
            "    result = mammoth.convert_to_markdown(docx_file)\n",
            "    md_text = result.value\n",
            "\n",
            "# Break the markdown text into segments\n",
            "segments = textwrap.wrap(md_text, 2048)\n",
            "\n",
            "# Proofread each segment\n",
            "proofread_segments = []\n",
            "prompt_format = \"please perform copy editing on this text to correct spelling, grammar, usage, and punctuation mistakes: {}\"\n",
            "\n",
            "for segment in segments:\n",
            "    prompt = prompt_format.format(segment)\n",
            "    response = openai.Completion.create(\n",
            "      engine=\"text-davinci-003\",\n",
            "      prompt=prompt,\n",
            "      temperature=0.5,\n",
            "      max_tokens=2100\n",
            "    )\n",
            "    # Save the corrected text\n",
            "    proofread_segments.append(response.choices[0].text.strip())\n",
            "\n",
            "# Reassemble the proofread markdown\n",
            "proofread_md = ' '.join(proofread_segments)\n",
            "\n",
            "# Write the proofread markdown to a file\n",
            "with open('proofread.md', 'w') as file:\n",
            "    file.write(proofread_md)\n",
            "\n",
            "# using pandoc to convert the markdown back to word document.\n",
            "os.system(\"pandoc -s proofread.md -o proofread.docx\")\n",
            "\n",
            "print(\"Proofreading is complete. The proofread document is 'proofread.docx'\")\n",
            "\n",
            "```\n",
            "## Please replace 'YOUR_OPENAI_API_KEY' with your real OpenAI API key before running. \n",
            "\n",
            "Please note GPT-4 doesn't exist at the time of writing this, thus mentioned **text-davinci-003** which is an upgraded version of **text-davinci-002** as per the information mentioned by OpenAI.\n",
            "\n",
            "Please install all required libraries before running the script:\n",
            "\n",
            "```sh\n",
            "pip install openai pypandoc python-docx mammoth\n",
            "```\n",
            "The above python program should be saved in a file and then can be run from the command line:\n",
            "```sh\n",
            "python proofread.py\n",
            "```\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Provide feedback to assistant. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: exit\n"
          ]
        }
      ]
    }
  ]
}